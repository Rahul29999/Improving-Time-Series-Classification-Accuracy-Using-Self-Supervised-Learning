{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Improving Time Series Classification Accuracy Using Self-Supervised Learning.**\n"
      ],
      "metadata": {
        "id": "eGZ0lVoyWjxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import data and preprocessing**"
      ],
      "metadata": {
        "id": "ugFvw06RT_fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Step 1: Extract files from Gesture.zip\n",
        "with zipfile.ZipFile('/content/Gesture.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/Gesture_data')\n",
        "\n",
        "# Step 2: Load each extracted .pt file and access 'samples' and 'labels'\n",
        "train_data = torch.load('/content/Gesture_data/train.pt')\n",
        "val_data = torch.load('/content/Gesture_data/val.pt')\n",
        "test_data = torch.load('/content/Gesture_data/test.pt')\n",
        "\n",
        "# Extract data and labels using 'samples' and 'labels' keys\n",
        "X_train = train_data['samples']\n",
        "y_train = train_data['labels']\n",
        "X_val = val_data['samples']\n",
        "y_val = val_data['labels']\n",
        "X_test = test_data['samples']\n",
        "y_test = test_data['labels']\n",
        "\n",
        "# Check the shapes of tensors\n",
        "print(\"Train Data Shape:\", X_train.shape)\n",
        "print(\"Train Labels Shape:\", y_train.shape)\n",
        "print(\"Validation Data Shape:\", X_val.shape)\n",
        "print(\"Validation Labels Shape:\", y_val.shape)\n",
        "print(\"Test Data Shape:\", X_test.shape)\n",
        "print(\"Test Labels Shape:\", y_test.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "hEUcL_a7tC_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e5a3a92-2292-4252-f352-8241f2e068af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Shape: torch.Size([320, 3, 206])\n",
            "Train Labels Shape: torch.Size([320])\n",
            "Validation Data Shape: torch.Size([120, 3, 206])\n",
            "Validation Labels Shape: torch.Size([120])\n",
            "Test Data Shape: torch.Size([120, 3, 206])\n",
            "Test Labels Shape: torch.Size([120])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-657871ab67cf>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  train_data = torch.load('/content/Gesture_data/train.pt')\n",
            "<ipython-input-4-657871ab67cf>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  val_data = torch.load('/content/Gesture_data/val.pt')\n",
            "<ipython-input-4-657871ab67cf>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  test_data = torch.load('/content/Gesture_data/test.pt')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Self-Supervised Learning Model(e.g., Autoencoder)**"
      ],
      "metadata": {
        "id": "2IJZznfZkezX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the autoencoder model\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv1d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose1d(32, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose1d(16, 3, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "input_dim = X_train.shape[-1]\n",
        "autoencoder = Autoencoder(input_dim)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "gCzCC-3DNFFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train the Self-Supervised Model**"
      ],
      "metadata": {
        "id": "rjfIrvVbk1jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure input tensors are in float32 format\n",
        "X_train = X_train.float()\n",
        "X_val = X_val.float()\n",
        "X_test = X_test.float()\n",
        "\n",
        "# Training the autoencoder\n",
        "num_epochs = 20\n",
        "batch_size = 32\n",
        "train_loader = torch.utils.data.DataLoader(X_train, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for data in train_loader:\n",
        "        # Convert data to float32 if necessary\n",
        "        data = data.float()\n",
        "\n",
        "        # Forward pass\n",
        "        output = autoencoder(data)\n",
        "        loss = criterion(output, data)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaWggAtKOXMk",
        "outputId": "18806ffa-d13f-440d-ff3e-7437181dd288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 1.1144\n",
            "Epoch [2/20], Loss: 0.9769\n",
            "Epoch [3/20], Loss: 0.7050\n",
            "Epoch [4/20], Loss: 0.6698\n",
            "Epoch [5/20], Loss: 0.5889\n",
            "Epoch [6/20], Loss: 0.5780\n",
            "Epoch [7/20], Loss: 0.5352\n",
            "Epoch [8/20], Loss: 0.5281\n",
            "Epoch [9/20], Loss: 0.5699\n",
            "Epoch [10/20], Loss: 0.5305\n",
            "Epoch [11/20], Loss: 0.5328\n",
            "Epoch [12/20], Loss: 0.5567\n",
            "Epoch [13/20], Loss: 0.4820\n",
            "Epoch [14/20], Loss: 0.5041\n",
            "Epoch [15/20], Loss: 0.4797\n",
            "Epoch [16/20], Loss: 0.4494\n",
            "Epoch [17/20], Loss: 0.5402\n",
            "Epoch [18/20], Loss: 0.5741\n",
            "Epoch [19/20], Loss: 0.5199\n",
            "Epoch [20/20], Loss: 0.5724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extract Features for Classification**"
      ],
      "metadata": {
        "id": "42W6xkvpk_a2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "autoencoder.eval()\n",
        "\n",
        "# Extract features for each dataset using the encoder part of the autoencoder\n",
        "with torch.no_grad():\n",
        "    X_train_features = autoencoder.encoder(X_train).view(X_train.size(0), -1)\n",
        "    X_val_features = autoencoder.encoder(X_val).view(X_val.size(0), -1)\n",
        "    X_test_features = autoencoder.encoder(X_test).view(X_test.size(0), -1)\n",
        "\n",
        "print(\"Train Features Shape:\", X_train_features.shape)\n",
        "print(\"Validation Features Shape:\", X_val_features.shape)\n",
        "print(\"Test Features Shape:\", X_test_features.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJXdzhnXOdH_",
        "outputId": "069d2ef3-330d-445f-f93b-6aac8f336d8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Features Shape: torch.Size([320, 6592])\n",
            "Validation Features Shape: torch.Size([120, 6592])\n",
            "Test Features Shape: torch.Size([120, 6592])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train a Classifier Using Extracted Features**"
      ],
      "metadata": {
        "id": "X0hIGwGSlF2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Convert features and labels to numpy arrays for use with sklearn\n",
        "X_train_features_np = X_train_features.cpu().numpy()\n",
        "y_train_np = y_train.cpu().numpy()\n",
        "X_val_features_np = X_val_features.cpu().numpy()\n",
        "y_val_np = y_val.cpu().numpy()\n",
        "X_test_features_np = X_test_features.cpu().numpy()\n",
        "y_test_np = y_test.cpu().numpy()\n",
        "\n",
        "# Train a Random Forest classifier\n",
        "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "classifier.fit(X_train_features_np, y_train_np)\n",
        "\n",
        "# Evaluate the classifier on the test set\n",
        "y_pred = classifier.predict(X_test_features_np)\n",
        "accuracy = accuracy_score(y_test_np, y_pred)\n",
        "print(\"Classification Accuracy on Test Set:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAGTi9_1PVBl",
        "outputId": "e1b00007-717e-4d37-c578-25f99c87c2b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Accuracy on Test Set: 0.6583333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate Performance**"
      ],
      "metadata": {
        "id": "Y90ecPtWlPxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Generate a detailed classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_np, y_pred))\n",
        "\n",
        "# Display the confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_np, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUad2xe4PYZK",
        "outputId": "69514f73-3515-4f95-cfe3-89c256a4e7b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.73      0.73      0.73        15\n",
            "         1.0       0.80      0.80      0.80        15\n",
            "         2.0       0.87      0.87      0.87        15\n",
            "         3.0       0.62      0.67      0.65        15\n",
            "         4.0       0.40      0.13      0.20        15\n",
            "         5.0       0.17      0.20      0.18        15\n",
            "         6.0       0.78      0.93      0.85        15\n",
            "         7.0       0.78      0.93      0.85        15\n",
            "\n",
            "    accuracy                           0.66       120\n",
            "   macro avg       0.64      0.66      0.64       120\n",
            "weighted avg       0.64      0.66      0.64       120\n",
            "\n",
            "Confusion Matrix:\n",
            " [[11  1  0  0  0  2  0  1]\n",
            " [ 0 12  0  0  1  0  2  0]\n",
            " [ 0  0 13  0  0  2  0  0]\n",
            " [ 1  0  0 10  0  3  0  1]\n",
            " [ 3  0  0  3  2  7  0  0]\n",
            " [ 0  2  2  3  2  3  1  2]\n",
            " [ 0  0  0  0  0  1 14  0]\n",
            " [ 0  0  0  0  0  0  1 14]]\n"
          ]
        }
      ]
    }
  ]
}
